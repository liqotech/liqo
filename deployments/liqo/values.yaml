# Default values for liqo.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# -- Images' tag to select a development version of liqo instead of a release
tag: ""
# -- The pullPolicy for liqo pods
pullPolicy: "IfNotPresent"
apiServer:
  # -- The address that must be used to contact your API server, it needs to be reachable from the clusters that you will peer with (defaults to your master IP)
  address: ""
  # -- Indicates that the API Server is exposing a certificate issued by a trusted Certification Authority
  trustedCA: false

controllerManager:
  # -- The number of controller-manager instances to run, which can be increased for active/passive high availability.
  replicas: 1
  pod:
    # -- controller-manager pod annotations
    annotations: {}
    # -- controller-manager pod labels
    labels: {}
    # -- controller-manager pod extra arguments
    extraArgs: []
    # -- controller-manager pod containers' resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/)
    resources:
      limits: {}
      requests: {}
  # -- controller-manager image repository
  imageName: "ghcr.io/liqotech/liqo-controller-manager"
  config:
    # -- It defines the percentage of available cluster resources that you are willing to share with foreign clusters.
    resourceSharingPercentage: 30
    # -- the threshold (in percentage) of resources quantity variation which triggers a ResourceOffer update.
    offerUpdateThresholdPercentage: ""
    # -- The address of an external resource plugin service (see https://github.com/liqotech/liqo-resource-plugins for additional information), overriding the default resource computation logic based on the percentage of available resources. Leave it empty to use the standard local resource monitor.
    resourcePluginAddress: ""
    # -- It enforces offerer-side that offloaded pods do not exceed offered resources (based on container limits).
    # This feature is suggested to be enabled when consumer-side enforcement is not sufficient.
    # It has the same tradeoffs of resource quotas (i.e, it requires all offloaded pods to have resource limits set).
    enableResourceEnforcement: false
    # -- Ensure offloaded pods running on a failed node are evicted and rescheduled on a healthy node, preventing them to remain in a terminating state indefinitely.
    # This feature can be useful in case of remote node failure to guarantee better service continuity and to have the expected pods workload on the remote cluster.
    # However, enabling this feature could produce zombies in the worker node, in case the node returns Ready again without a restart
    enableNodeFailureController: false

route:
  pod:
    # -- route pod annotations
    annotations: {}
    # -- route pod labels
    labels: {}
    # -- route pod extra arguments
    extraArgs: []
    # -- route pod containers' resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/)
    resources:
      limits: {}
      requests: {}
  # -- route image repository
  imageName: "ghcr.io/liqotech/liqonet"

gateway:
  # -- The number of gateway instances to run.
  # The gateway component supports active/passive high availability.
  # Make sure that there are enough nodes to accommodate the replicas, because being the instances in host network no more
  # than one replica can be scheduled on a given node.
  replicas: 1
  pod:
    # -- gateway pod annotations
    annotations: {}
    # -- gateway pod labels
    labels: {}
    # -- gateway pod extra arguments
    extraArgs: []
    # -- gateway pod containers' resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/)
    resources:
      limits: {}
      requests: {}
  # -- gateway image repository
  imageName: "ghcr.io/liqotech/liqonet"
  service:
    # -- If you plan to use liqo over the Internet, consider to change this field to "LoadBalancer".
    # Instead, if your nodes are directly reachable from the cluster you are peering to, you may change it to "NodePort".
    type: "LoadBalancer"
    annotations: {}
  config:
    # -- Override the default address where your service is available, you should configure it if behind a reverse proxy or NAT.
    addressOverride: ""
    # -- Overrides the port where your service is available, you should configure it if behind a reverse proxy or NAT and is different from the listening port.
    portOverride: ""
    # -- port used by the vpn tunnel.
    listeningPort: 5871
  metrics:
    # -- expose metrics about network traffic towards cluster peers.
    enabled: false
    # -- port used to expose metrics.
    port: 5872
    serviceMonitor:
      # -- create a prometheus servicemonitor.
      enabled: false
      # -- setup service monitor requests interval. If empty, Prometheus uses the global scrape interval.
      # ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint
      interval: ""
      # -- setup service monitor scrape timeout. If empty, Prometheus uses the global scrape timeout.
      # ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint
      scrapeTimeout: ""

networkManager:
  pod:
    # -- networkManager pod annotations
    annotations: {}
    # -- networkManager pod labels
    labels: {}
    # -- networkManager pod extra arguments
    extraArgs: []
    # -- networkManager pod containers' resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/)
    resources:
      limits: {}
      requests: {}
  # -- networkManager image repository
  imageName: "ghcr.io/liqotech/liqonet"
  config:
    # -- The subnet used by the cluster for the pods, in CIDR notation
    podCIDR: ""
    # -- The subnet used by the cluster for the services, in CIDR notation
    serviceCIDR: ""
    # -- Usually the IPs used for the pods in k8s clusters belong to private subnets.
    # In order to prevent IP conflicting between locally used private subnets in your infrastructure and private subnets belonging to remote clusters
    # you need tell liqo the subnets used in your cluster. E.g if your cluster nodes belong to the 192.168.2.0/24 subnet then
    # you should add that subnet to the reservedSubnets. PodCIDR and serviceCIDR used in the local cluster are automatically added to the reserved list.
    reservedSubnets: []
    # -- Set of additional network pools.
    # Network pools are used to map a cluster network into another one in order to prevent conflicts.
    # Default set of network pools is: [10.0.0.0/8, 192.168.0.0/16, 172.16.0.0/12]
    additionalPools: []

crdReplicator:
  pod:
    # -- crdReplicator pod annotations
    annotations: {}
    # -- crdReplicator pod labels
    labels: {}
    # -- crdReplicator pod extra arguments
    extraArgs: []
    # -- crdReplicator pod containers' resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/)
    resources:
      limits: {}
      requests: {}
  # -- crdReplicator image repository
  imageName: "ghcr.io/liqotech/crd-replicator"

discovery:
  pod:
    # -- discovery pod annotations
    annotations: {}
    # -- discovery pod labels
    labels: {}
    # -- discovery pod extra arguments
    extraArgs: []
    # -- discovery pod containers' resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/)
    resources:
      limits: {}
      requests: {}
  # -- discovery image repository
  imageName: "ghcr.io/liqotech/discovery"
  config:
    # -- Specify an unique ID (must be a valid uuidv4) for your cluster, instead of letting helm generate it automatically at install time. You can generate it using the command: `uuidgen`
    # Setting this field is necessary when using tools such as ArgoCD, since the helm lookup function is not supported and a new value would be generated at each deployment.
    clusterIDOverride: ""
    # -- Set a mnemonic name for your cluster
    clusterName: ""
    # -- A set of labels which characterizes the local cluster when exposed remotely as a virtual node.
    # It is suggested to specify the distinguishing characteristics that may be used to decide whether to offload pods on this cluster.
    clusterLabels: {}
     # topology.kubernetes.io/zone: us-east-1
     # liqo.io/provider: your-provider

    # -- Automatically join discovered clusters
    autojoin: true
    # -- Allow (by default) the remote clusters to establish a peering with our cluster
    incomingPeeringEnabled: true
    # -- Enable the mDNS advertisement on LANs, set to false to not be discoverable from other clusters in the same LAN
    enableAdvertisement: false
    # -- Enable the mDNS discovery on LANs, set to false to not look for other clusters available in the same LAN
    enableDiscovery: false
    # -- Time-to-live before an automatically discovered clusters is deleted from the list of available ones if no longer announced (in seconds)
    ttl: 90

auth:
  pod:
    # -- auth pod annotations
    annotations: {}
    # -- auth pod labels
    labels: {}
    # -- auth pod extra arguments
    extraArgs: []
    # -- auth pod containers' resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/)
    resources:
      limits: {}
      requests: {}
  # -- auth image repository
  imageName: "ghcr.io/liqotech/auth-service"
  initContainer:
    # -- auth init container image repository
    imageName: "ghcr.io/liqotech/cert-creator"
  service:
    # -- The type of service used to expose the Authentication Service.
    # If you are exposing this service with an Ingress, you can change it to ClusterIP;
    # if your cluster does not support LoadBalancer services, consider to switch it to NodePort.
    # See https://doc.liqo.io/installation/ for more details.
    type: "LoadBalancer"
    # -- auth service annotations
    annotations: {}
  # -- Enable TLS for the Authentication Service Pod (using a self-signed certificate).
  # If you are exposing this service with an Ingress consider to disable it or add the appropriate annotations to the Ingress resource.
  tls: true
  ingress:
    # -- Auth ingress annotations
    annotations: {}
    # -- Whether to enable the creation of the Ingress resource
    enable: false
    # -- Set the hostname for your ingress
    host: ""
    # -- Set your ingress class
    class: ""
  config:
    # -- Set to false to disable the authentication of discovered clusters. NB: use it only for testing installations
    enableAuthentication: true
    # -- Override the default address where your service is available, you should configure it if behind a reverse proxy or NAT.
    addressOverride: ""
    # -- Overrides the port where your service is available, you should configure it if behind a reverse proxy or NAT or using an Ingress with a port different from 443.
    portOverride: ""

metricAgent:
  # -- Enable the metric agent
  enable: true
  pod:
    # -- metricAgent pod annotations
    annotations: {}
    # -- metricAgent pod labels
    labels: {}
    # -- metricAgent pod extra arguments
    extraArgs: []
    # -- metricAgent pod containers' resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/)
    resources:
      limits: {}
      requests: {}
  # -- metricAgent image repository
  imageName: "ghcr.io/liqotech/metric-agent"
  initContainer:
    # -- auth init container image repository
    imageName: "ghcr.io/liqotech/cert-creator"

telemetry:
  # -- Enable the telemetry collector
  enable: true
  pod:
    # -- telemetry pod annotations
    annotations: {}
    # -- telemetry pod labels
    labels: {}
    # -- telemetry pod extra arguments
    extraArgs: []
    # -- telemetry pod containers' resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/)
    resources:
      limits: {}
      requests: {}
  # -- telemetry image repository
  imageName: "ghcr.io/liqotech/telemetry"
  config:
    # -- Set the schedule of the telemetry collector CronJob
    schedule: ""
    # schedule: "0 */12 * * *"

webhook:
  # -- the port the webhook server binds to
  port: 9443
  # -- the webhook failure policy, among Ignore and Fail
  failurePolicy: Fail
  patch:
    # -- the image used for the patch jobs to manage certificates
    image: k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1

virtualKubelet:
  # -- virtual kubelet image repository
  imageName: "ghcr.io/liqotech/virtual-kubelet"
  # add additional values for this fields to add to virtual kubelet deployments and pods
  extra:
    # -- virtual kubelet pod extra annotations
    annotations: {}
    # -- virtual kubelet pod extra labels
    labels: {}
    # -- virtual kubelet pod extra arguments
    args: []
    # -- virtual kubelet pod containers' resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/)
    resources:
      limits: {}
      requests: {}
  virtualNode:
    extra:
      # -- virtual node extra annotations
      annotations: {}
      # -- virtual node extra labels
      labels: {}

uninstaller:
  pod:
    # -- uninstaller pod annotations
    annotations: {}
    # -- uninstaller pod labels
    labels: {}
    # -- uninstaller pod extra arguments
    extraArgs: []
    # -- uninstaller pod containers' resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/)
    resources:
      limits: {}
      requests: {}
  # -- uninstaller image repository
  imageName: "ghcr.io/liqotech/uninstaller"

proxy:
  pod:
    # -- proxy pod annotations
    annotations: {}
    # -- proxy pod labels
    labels: {}
    # -- proxy pod extra arguments
    extraArgs: []
    # -- proxy pod containers' resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/)
    resources:
      limits: {}
      requests: {}
  # -- proxy image repository
  imageName: "envoyproxy/envoy:v1.21.0"
  service:
    type: "ClusterIP"
    annotations: {}
  config:
    # -- port used by envoy proxy
    listeningPort: 8118

storage:
  # -- enable the liqo virtual storage class on the local cluster. You will be able to
  # offload your persistent volumes and other clusters will be able to schedule their
  # persistent workloads on the current cluster.
  enable: true
  # -- name to assign to the liqo virtual storage class
  virtualStorageClassName: liqo
  # -- name of the real storage class to use in the local cluster
  realStorageClassName: ""
  # -- namespace where liqo will deploy specific PVCs
  storageNamespace: liqo-storage

# -- liqo name override
nameOverride: ""
# -- full liqo name override
fullnameOverride: ""

# aws configuration for the local cluster and the Liqo user,
# this user should be able to create new IAM user, to create new programmatic access
# credentials, and to describe EKS clusters.
# NOTE: set it only if running on EKS, otherwise let this fields with the default value
awsConfig:
  # -- accessKeyID for the Liqo user
  accessKeyId: ""
  # -- secretAccessKey for the Liqo user
  secretAccessKey: ""
  # -- AWS region where the clsuter is runnnig
  region: ""
  # -- name of the EKS cluster
  clusterName: ""

# set the OpenShift-specific configurations
openshiftConfig:
  # -- enable the OpenShift support
  enable: false
  # -- the security context configurations granted to the virtual kubelet in the local cluster.
  # The configuration of one or more SCCs for the virtual kubelet is not strictly required, and privileges can be reduced in production environments.
  # Still, the default configuration (i.e., anyuid) is suggested to prevent problems (i.e., the virtual kubelet fails to add the appropriate labels) when
  # attempting to offload pods not managed by higher-level abstractions (e.g., Deployments), and not associated with a properly privileged service account.
  # Indeed, "anyuid" is the SCC automatically associated with pods created by cluster administrators.
  # Any pod granted a more privileged SCC and not linked to an adequately privileged service account will fail to be offloaded.
  virtualKubeletSCCs:
  - anyuid

# configuration for liqo networking
networkConfig:
  # -- set the mtu for the interfaces managed by liqo: vxlan, tunnel and veth interfaces
  # The value is used by the gateway and route operators.
  # The default value is configured to ensure correct functioning regardless of the combination of the underlying environments
  # (e.g., cloud providers). This guarantees improved compatibility at the cost of possible limited performance drops.
  mtu: 1340
