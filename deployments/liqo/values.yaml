# Default values for liqo.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# -- Images' tag to select a development version of liqo instead of a release
tag: ""
# -- The pullPolicy for liqo pods.
pullPolicy: "IfNotPresent"

common:
  # -- NodeSelector for all liqo pods, excluding virtual kubelet.
  nodeSelector: {}
  # -- Tolerations for all liqo pods, excluding virtual kubelet.
  tolerations: []
  # -- Affinity for all liqo pods, excluding virtual kubelet.
  affinity: {}
  # -- Extra arguments for all liqo pods, excluding virtual kubelet.
  extraArgs: []

apiServer:
  # -- The address that must be used to contact your API server, it needs to be reachable from the clusters that you will peer with (defaults to your master IP).
  address: ""
  # -- Indicates that the API Server is exposing a certificate issued by a trusted Certification Authority.
  trustedCA: false

networking:
  # -- Use the default Liqo network manager.
  internal: true
  # -- Reflect pod IPs and EnpointSlices to the remote clusters.
  reflectIPs: true
  # -- Iptables configuration tuning.
  iptables:
    # -- Select the iptables mode to use. Possible values are "legacy" and "nf_tables".
    mode: "nf_tables"
  # -- Set the MTU for the interfaces managed by liqo: vxlan, tunnel and veth interfaces.
  # The value is used by the gateway and route operators.
  # The default value is configured to ensure correct behavior regardless of the combination of the underlying environments
  # (e.g., cloud providers). This guarantees improved compatibility at the cost of possible limited performance drops.
  mtu: 1340
  # -- Select the mode to enforce security on connectivity among clusters. Possible values are "FullPodToPod" and "IntraClusterTrafficSegregation" 
  securityMode: "FullPodToPod"

reflection:
  skip:
    # -- List of labels that must not be reflected on remote clusters.
    labels: []
    # -- List of annotations that must not be reflected on remote clusters.
    annotations: [
      cloud.google.com/neg,
      cloud.google.com/neg-status,
      kubernetes.digitalocean.com/load-balancer-id,
      ingress.kubernetes.io/backends,
      ingress.kubernetes.io/forwarding-rule,
      ingress.kubernetes.io/target-proxy,
      ingress.kubernetes.io/url-map,
      metallb.universe.tf/address-pool,
      metallb.universe.tf/ip-allocated-from-pool,
      metallb.universe.tf/loadBalancerIPs,
    ]
  pod:
    # -- The number of workers used for the pods reflector. Set 0 to disable the reflection of pods.
    workers: 10
  service:
    # -- The number of workers used for the services reflector. Set 0 to disable the reflection of services.
    workers: 3
    # -- The type of reflection used for the services reflector. Ammitted values: "DenyList", "AllowList".
    type: DenyList
  endpointslice:
    # -- The number of workers used for the endpointslices reflector. Set 0 to disable the reflection of endpointslices.
    workers: 10
  ingress:
    # -- The number of workers used for the ingresses reflector. Set 0 to disable the reflection of ingresses.
    workers: 3
    # -- The type of reflection used for the ingresses reflector. Ammitted values: "DenyList", "AllowList".
    type: DenyList
  configmap:
    # -- The number of workers used for the configmaps reflector. Set 0 to disable the reflection of configmaps.
    workers: 3
    # -- The type of reflection used for the configmaps reflector. Ammitted values: "DenyList", "AllowList".
    type: DenyList
  secret:
    # -- The number of workers used for the secrets reflector. Set 0 to disable the reflection of secrets.
    workers: 3
    # -- The type of reflection used for the secrets reflector. Ammitted values: "DenyList", "AllowList".
    type: DenyList
  serviceaccount:
    # -- The number of workers used for the serviceaccounts reflector. Set 0 to disable the reflection of serviceaccounts.
    workers: 3
  persistentvolumeclaim:
    # -- The number of workers used for the persistentvolumeclaims reflector. Set 0 to disable the reflection of persistentvolumeclaims.
    workers: 3
  event:
    # -- The number of workers used for the events reflector. Set 0 to disable the reflection of events.
    workers: 3
    # -- The type of reflection used for the events reflector. Ammitted values: "DenyList", "AllowList".
    type: DenyList

controllerManager:
  # -- The number of controller-manager instances to run, which can be increased for active/passive high availability.
  replicas: 1
  pod:
    # -- Annotations for the controller-manager pod.
    annotations: {}
    # -- Labels for the controller-manager pod.
    labels: {}
    # -- Extra arguments for the controller-manager pod.
    extraArgs: []
    # -- Resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/) for the controller-manager pod.
    resources:
      limits: {}
      requests: {}
  # -- Image repository for the controller-manager pod.
  imageName: "ghcr.io/liqotech/liqo-controller-manager"
  config:
    # -- Percentage of available cluster resources that you are willing to share with foreign clusters.
    resourceSharingPercentage: 30
    # -- Threshold (in percentage) of the variation of resources that triggers a ResourceOffer update. E.g., when the available resources grow/decrease by X, a new ResourceOffer is generated.
    offerUpdateThresholdPercentage: ""
    # -- The address of an external resource plugin service (see https://github.com/liqotech/liqo-resource-plugins for additional information), overriding the default resource computation logic based on the percentage of available resources. Leave it empty to use the standard local resource monitor.
    resourcePluginAddress: ""
    # -- It enforces offerer-side that offloaded pods do not exceed offered resources (based on container limits).
    # This feature is suggested to be enabled when consumer-side enforcement is not sufficient.
    # It has the same tradeoffs of resource quotas (i.e, it requires all offloaded pods to have resource limits set).
    enableResourceEnforcement: false
    # -- Ensure offloaded pods running on a failed node are evicted and rescheduled on a healthy node, preventing them to remain in a terminating state indefinitely.
    # This feature can be useful in case of remote node failure to guarantee better service continuity and to have the expected pods workload on the remote cluster.
    # However, enabling this feature could produce zombies in the worker node, in case the node returns Ready again without a restart.
    enableNodeFailureController: false

route:
  pod:
    # -- Annotations for the route pod.
    annotations: {}
    # -- Labels for the route pod.
    labels: {}
    # -- Extra arguments for the route pod.
    extraArgs: []
    # -- Resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/) for the route pod.
    resources:
      limits: {}
      requests: {}
  # -- Image repository for the route pod.
  imageName: "ghcr.io/liqotech/liqonet"
  # -- Extra tolerations for the route daemonset.
  tolerations: []

gateway:
  # -- The number of gateway instances to run.
  # The gateway component supports active/passive high availability.
  # Make sure that there are enough nodes to accommodate the replicas, because such pod has to run in the host network, hence
  # no more than one replica can be scheduled on a given node.
  replicas: 1
  pod:
    # -- Annotations for the network gateway pod.
    annotations: {}
    # -- Labels for the network gateway pod.
    labels: {}
    # -- Extra arguments for the network gateway pod.
    extraArgs: []
    # -- Resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/) for the network gateway pod.
    resources:
      limits: {}
      requests: {}
  # -- Image repository for the network gateway pod.
  imageName: "ghcr.io/liqotech/liqonet"
  service:
    # -- Kubernetes service to be used to expose the network gateway pod.
    # If you plan to use liqo over the Internet, consider to change this field to "LoadBalancer".
    # Instead, if your nodes are directly reachable from the cluster you are peering to, you may change it to "NodePort".
    type: "LoadBalancer"
    # -- Annotations for the network gateway service.
    annotations: {}
    # -- Labels for the network gateway service.
    labels: {}
    # -- Options valid if service type is NodePort.
    nodePort:
      # -- Force the port used by the NodePort service.
      port: ""
    # -- Options valid if service type is LoadBalancer.
    loadBalancer:
      # -- Override the IP here if service type is LoadBalancer and you want to use a specific IP address, e.g., because you want a static LB.
      ip: ""
      # -- Set to false if you expose the gateway service as LoadBalancer and you do not want to create also a NodePort associated to it (Note: this setting is useful only on cloud providers that support this feature).
      allocateLoadBalancerNodePorts: ""
  config:
    # -- Override the default address where your network gateway service is available.
    # You should configure it if the network gateway is behind a reverse proxy or NAT.
    addressOverride: ""
    # -- Overrides the port where your network gateway service is available.
    # You should configure it if the network gateway is behind a reverse proxy or NAT and is different from the listening port.
    portOverride: ""
    # -- Port used by the network gateway.
    listeningPort: 5871
    # -- Implementation used by wireguard to establish the VPN tunnel between two clusters.
    # Possible values are "userspace" and "kernel". Do not use "userspace" unless strictly necessary 
    # (i.e., only if the Linux kernel does not support Wireguard).
    wireguardImplementation: "kernel"
  metrics:
    # -- Expose metrics about network traffic towards cluster peers.
    enabled: false
    # -- Port used to expose metrics.
    port: 5872
    # -- Service used to expose metrics.
    service:
      # -- Labels for the metrics service.
      labels: {}
      # -- Annotations for the metrics service.
      annotations: {}
    serviceMonitor:
      # -- Enable/Disable a Prometheus servicemonitor. Turn on this flag when the Prometheus Operator
      # runs in your cluster; otherwise simply export the port above as an external endpoint.
      enabled: false
      # -- Customize service monitor requests interval. If empty, Prometheus uses the global scrape interval
      # (https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint).
      interval: ""
      # -- Customize service monitor scrape timeout. If empty, Prometheus uses the global scrape timeout
      # (https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint).
      scrapeTimeout: ""
      # -- Labels for the gateway servicemonitor.
      labels: {}

networkManager:
  externalIPAM:
    # -- Use an external IPAM to allocate the IP addresses for the pods.
    enabled: false
    # -- The URL of the external IPAM.
    url: ""
  pod:
    # -- Annotations for the networkManager pod.
    annotations: {}
    # -- Labels for the networkManager pod.
    labels: {}
    # -- Extra arguments for the networkManager pod.
    extraArgs: []
    # -- Resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/) for the networkManager pod.
    resources:
      limits: {}
      requests: {}
  # -- Image repository for the networkManager pod.
  imageName: "ghcr.io/liqotech/liqonet"
  config:
    # -- The subnet used by the pods in your cluster, in CIDR notation (e.g., 10.0.0.0/16).
    podCIDR: ""
    # -- The subnet used by the services in you cluster, in CIDR notation (e.g., 172.16.0.0/16).
    serviceCIDR: ""
    # -- List of IP subnets that do not have to be used by Liqo.
    # Liqo can perform automatic IP address remapping when a remote cluster is peering with you, e.g., in case IP address spaces (e.g., PodCIDR) overlaps.
    # In order to prevent IP conflicting between locally used private subnets in your infrastructure and private subnets belonging to remote clusters
    # you need tell liqo the subnets used in your cluster. E.g if your cluster nodes belong to the 192.168.2.0/24 subnet, then
    # you should add that subnet to the reservedSubnets. PodCIDR and serviceCIDR used in the local cluster are automatically added to the reserved list.
    reservedSubnets: []
    # -- Set of additional network pools to perform the automatic address mapping in Liqo.
    # Network pools are used to map a cluster network into another one in order to prevent conflicts.
    # Default set of network pools is: [10.0.0.0/8, 192.168.0.0/16, 172.16.0.0/12]
    additionalPools: []

crdReplicator:
  pod:
    # -- Annotations for the crdReplicator pod.
    annotations: {}
    # -- Labels for the crdReplicator pod.
    labels: {}
    # -- Extra arguments for the crdReplicator pod.
    extraArgs: []
    # -- Resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/) for the crdReplicator pod.
    resources:
      limits: {}
      requests: {}
  # -- Image repository for the crdReplicator pod.
  imageName: "ghcr.io/liqotech/crd-replicator"

discovery:
  pod:
    # -- Annotation for the discovery pod.
    annotations: {}
    # -- Labels for the discovery pod.
    labels: {}
    # -- Extra arguments for the discovery pod.
    extraArgs: []
    # -- Resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/) for the discovery pod.
    resources:
      limits: {}
      requests: {}
  # -- Image repository for the discovery pod.
  imageName: "ghcr.io/liqotech/discovery"
  config:
    # -- Specify an unique ID (must be a valid uuidv4) for your cluster, instead of letting helm generate it automatically at install time.
    # You can generate it using the command: `uuidgen`
    # This field is needed when using tools such as ArgoCD, since the helm lookup function is not supported and a new value would be generated at each deployment.
    clusterIDOverride: ""
    # -- Set a mnemonic name for your cluster.
    clusterName: ""
    # -- A set of labels that characterizes the local cluster when exposed remotely as a virtual node.
    # It is suggested to specify the distinguishing characteristics that may be used to decide whether to offload pods on this cluster.
    clusterLabels: {}
     # topology.kubernetes.io/zone: us-east-1
     # liqo.io/provider: your-provider

    # -- Enable the mDNS discovery on LANs, set to false to not look for other clusters available in the same LAN.
    # Usually this feature should be active when you have multiple (tiny) clusters on the same LAN (e.g., multiple K3s running on individual devices);
    # if your clusters operate on the big Internet, this feature is not needed and it can be turned off.
    enableDiscovery: false
    # -- Enable the mDNS advertisement on LANs, set to false to not be discoverable from other clusters in the same LAN.
    # When this flag is 'false', the cluster can still receive the advertising from other (local) clusters, and automatically peer with them. 
    enableAdvertisement: false
    # -- Time-to-live before an automatically discovered clusters is deleted from the list of available ones if no longer announced (in seconds).
    ttl: 90
    # -- Automatically join discovered clusters.
    autojoin: true
    # -- Allow (by default) the remote clusters to establish a peering with our cluster.
    incomingPeeringEnabled: true

auth:
  pod:
    # -- Annotations for the auth pod.
    annotations: {}
    # -- Labels for the auth pod.
    labels: {}
    # -- Extra arguments for the auth pod.
    extraArgs: []
    # -- Resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/) for the auth pod.
    resources:
      limits: {}
      requests: {}
  # -- Image repository for the auth pod.
  imageName: "ghcr.io/liqotech/auth-service"
  initContainer:
    # -- Image repository for the init container of the auth pod.
    imageName: "ghcr.io/liqotech/cert-creator"
  service:
    # -- Kubernetes service used to expose the Authentication Service.
    # If you are exposing this service with an Ingress, you can change it to ClusterIP;
    # if your cluster does not support LoadBalancer services, consider to switch it to NodePort.
    # See https://doc.liqo.io/installation/ for more details.
    type: "LoadBalancer"
    # -- Labels for the auth service.
    labels: {}
    # -- Annotations for the auth service.
    annotations: {}
    # -- Port used by the Authentication Service.
    port: 443
    # -- Options valid if service type is NodePort.
    nodePort:
      # -- Force the port used by the NodePort service. This value must be included between 30000 and 32767.
      port: ""
    # -- Options valid if service type is LoadBalancer.
    loadBalancer:
      # -- Override the IP here if service type is LoadBalancer and you want to use a specific IP address, e.g., because you want a static LB.
      ip: ""
      # -- Set to false if you expose the gateway service as LoadBalancer and you do not want to create also a NodePort associated to it (Note: this setting is useful only on cloud providers that support this feature).
      allocateLoadBalancerNodePorts: ""
  # -- Enable TLS for the Authentication Service Pod (using a self-signed certificate).
  # If you are exposing this service with an Ingress, consider to disable it or add the appropriate annotations to the Ingress resource.
  tls: true
  ingress:
    # -- Annotations for the Auth ingress.
    annotations: {}
    # -- Enable/disable the creation of the Ingress resource.
    enable: false
    # -- Set the hostname for your ingress.
    host: ""
    # -- Set your ingress class.
    class: ""
    # -- Set port for your ingress.
    port: 443
    # -- Override default (ChartName-auth) tls secretName.
    tlsSecretName: ""
  config:
    # -- Set to false to disable the authentication of discovered clusters.
    # Note: use it only for testing installations.
    enableAuthentication: true
    # -- Override the default address where your service is available, you should configure it if behind a reverse proxy or NAT.
    addressOverride: ""
    # -- Overrides the port where your service is available, you should configure it if behind a reverse proxy or NAT or using an Ingress with a port different from 443.
    portOverride: ""

metricAgent:
  # -- Enable/Disable the virtual kubelet metric agent. This component aggregates all the kubelet-related metrics
  # (e.g., CPU, RAM, etc) collected on the nodes that are used by a remote cluster peered with you, then exporting 
  # the resulting values as a property of the virtual kubelet running on the remote cluster.
  enable: true
  config:
    # -- Set the timeout for the metrics server.
    timeout:
      read: 30s
      write: 30s
  pod:
    # -- Annotations for the metricAgent pod.
    annotations: {}
    # -- Labels for the metricAgent pod.
    labels: {}
    # -- Extra arguments for the metricAgent pod.
    extraArgs: []
    # -- Resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/) for the metricAgent pod.
    resources:
      limits: {}
      requests: {}
  # -- Image repository for the metricAgent pod.
  imageName: "ghcr.io/liqotech/metric-agent"
  initContainer:
    # --Image repository for the authentication init container for the metricAgent pod.
    imageName: "ghcr.io/liqotech/cert-creator"

telemetry:
  # -- Enable/Disable the telemetry collector.
  enable: true
  pod:
    # -- Annotations for the telemetry pod.
    annotations: {}
    # -- Labels for the telemetry pod.
    labels: {}
    # -- Extra arguments for the telemetry pod.
    extraArgs: []
    # -- Resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/) for the telemetry pod.
    resources:
      limits: {}
      requests: {}
  # -- Image repository for the telemetry pod.
  imageName: "ghcr.io/liqotech/telemetry"
  config:
    # -- Set the schedule of the telemetry collector CronJob. Consider setting this value on ArgoCD deployments to avoid randomization.
    schedule: ""
    # schedule: "0 */12 * * *"

webhook:
  # -- TCP port the webhook server binds to.
  port: 9443
  # -- Webhook failure policy, either Ignore or Fail.
  failurePolicy: Fail
  patch:
    # -- Image used for the patch jobs to manage certificates.
    image: k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1

virtualKubelet:
  # -- Image repository for the virtual kubelet.
  imageName: "ghcr.io/liqotech/virtual-kubelet"
  # Additional values that are added to virtual kubelet deployments and pods.
  extra:
    # -- Annotations for the virtual kubelet pod.
    annotations: {}
    # -- Labels for the virtual kubelet pod.
    labels: {}
    # -- Extra arguments virtual kubelet pod.
    args: []
    # -- Resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/) for the virtual kubelet pod.
    resources:
      limits: {}
      requests: {}
  virtualNode:
    extra:
      # -- Extra annotations for the virtual node.
      annotations: {}
      # -- Extra labels for the virtual node.
      labels: {}
  metrics:
    # -- Enable/Disable to expose metrics about virtual kubelet resources.
    enabled: false
    # -- Port used to expose metrics.
    port: 5872
    podMonitor:
      # -- Enable/Disable the creation of a Prometheus podmonitor. Turn on this flag when the Prometheus Operator
      # runs in your cluster; otherwise simply export the port above as an external endpoint.
      enabled: false
      # -- Setup pod monitor requests interval. If empty, Prometheus uses the global scrape interval
      # (https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint).
      interval: ""
      # -- Setup pod monitor scrape timeout. If empty, Prometheus uses the global scrape timeout
      # (https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint).
      scrapeTimeout: ""
      # -- Labels for the virtualkubelet podmonitor.
      labels: {}

uninstaller:
  pod:
    # -- Annotations for the uninstaller pod.
    annotations: {}
    # -- Labels for the uninstaller pod.
    labels: {}
    # -- Extra arguments for the uninstaller pod.
    extraArgs: []
    # -- Resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/) for the uninstaller pod.
    resources:
      limits: {}
      requests: {}
  # -- Image repository for the uninstaller pod.
  imageName: "ghcr.io/liqotech/uninstaller"

proxy:
  pod:
    # -- Annotations for the proxy pod.
    annotations: {}
    # -- Labels for the proxy pod.
    labels: {}
    # -- Extra arguments for the proxy pod.
    extraArgs: []
    # -- Resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/) for the proxy pod.
    resources:
      limits: {}
      requests: {}
  # -- Image repository for the proxy pod.
  imageName: "ghcr.io/liqotech/proxy"
  service:
    type: "ClusterIP"
    annotations: {}
  config:
    # -- Port used by the proxy pod.
    listeningPort: 8118

storage:
  # -- Enable/Disable the liqo virtual storage class on the local cluster. You will be able to
  # offload your persistent volumes, while other clusters will be able to schedule their
  # persistent workloads on the current cluster.
  enable: true
  # -- Name to assign to the liqo virtual storage class.
  virtualStorageClassName: liqo
  # -- Name of the real storage class to use in the local cluster.
  realStorageClassName: ""
  # -- Namespace where liqo will deploy specific PVCs. Internal parameter, do not change.
  storageNamespace: liqo-storage

# -- Override the standard name used by Helm and associated to Kubernetes/Liqo resources.
nameOverride: ""
# -- Override the standard full name used by Helm and associated to Kubernetes/Liqo resources.
fullnameOverride: ""

# AWS-specific configuration for the local cluster and the Liqo user.
# This user should be able (1) to create new IAM users, (2) to create new programmatic access
# credentials, and (3) to describe EKS clusters.
# NOTE: set it only if running on EKS, otherwise let this fields with the default value
awsConfig:
  # -- AccessKeyID for the Liqo user.
  accessKeyId: ""
  # -- SecretAccessKey for the Liqo user.
  secretAccessKey: ""
  # -- AWS region where the clsuter is runnnig.
  region: ""
  # -- Name of the EKS cluster.
  clusterName: ""

# OpenShift-specific configurations.
openshiftConfig:
  # -- Enable/Disable the OpenShift support, enabling Openshift-specific resources,
  # and setting the pod security contexts in a way that is compatible with Openshift.
  enable: false
  # -- Security context configurations granted to the virtual kubelet in the local cluster.
  # The configuration of one or more SCCs for the virtual kubelet is not strictly required, and privileges can be reduced in production environments.
  # Still, the default configuration (i.e., anyuid) is suggested to prevent problems (i.e., the virtual kubelet fails to add the appropriate labels) when
  # attempting to offload pods not managed by higher-level abstractions (e.g., Deployments), and not associated with a properly privileged service account.
  # Indeed, "anyuid" is the SCC automatically associated with pods created by cluster administrators.
  # Any pod granted a more privileged SCC and not linked to an adequately privileged service account will fail to be offloaded.
  virtualKubeletSCCs:
  - anyuid


